{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d5afc8-8486-45f2-b2b1-5b1d7aa7135e",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a18fab-8c9c-47b5-815e-5aea6f853959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm, trange\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ddef26-399f-4798-a1e1-a10b59896077",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0486a4d3-6094-4699-a2df-13c841910e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALNUM_CHARSET = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "\n",
    "def convert_to_tokens(indices, tokenizer, extended=False, extra_values_pos=None, strip=True):\n",
    "    if extended:\n",
    "        res = [tokenizer.convert_ids_to_tokens([idx])[0] if idx < len(tokenizer) else \n",
    "               (f\"[pos{idx-len(tokenizer)}]\" if idx < extra_values_pos else f\"[val{idx-extra_values_pos}]\") \n",
    "               for idx in indices]\n",
    "    else:\n",
    "        res = tokenizer.convert_ids_to_tokens(indices)\n",
    "    if strip:\n",
    "        res = list(map(lambda x: x[1:] if x[0] == 'Ġ' else \"#\" + x, res))\n",
    "    return res\n",
    "\n",
    "\n",
    "def top_tokens(v, k=100, tokenizer=None, only_alnum=False, only_ascii=True, with_values=False, \n",
    "               exclude_brackets=False, extended=True, extra_values=None, only_from_list=None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = my_tokenizer\n",
    "    v = deepcopy(v)\n",
    "    ignored_indices = []\n",
    "    if only_ascii:\n",
    "        ignored_indices.extend([key for val, key in tokenizer.vocab.items() if not val.strip('Ġ▁').isascii()])\n",
    "    if only_alnum: \n",
    "        ignored_indices.extend([key for val, key in tokenizer.vocab.items() if not (set(val.strip('Ġ▁[] ')) <= ALNUM_CHARSET)])\n",
    "    if only_from_list:\n",
    "        ignored_indices.extend([key for val, key in tokenizer.vocab.items() if val.strip('Ġ▁ ').lower() not in only_from_list])\n",
    "    if exclude_brackets:\n",
    "        ignored_indices = set(ignored_indices).intersection(\n",
    "            {key for val, key in tokenizer.vocab.items() if not (val.isascii() and val.isalnum())})\n",
    "        ignored_indices = list(ignored_indices)\n",
    "        \n",
    "    ignored_indices = list(set(ignored_indices))\n",
    "    v[ignored_indices] = -np.inf\n",
    "    extra_values_pos = len(v)\n",
    "    if extra_values is not None:\n",
    "        v = torch.cat([v, extra_values])\n",
    "    values, indices = torch.topk(v, k=k)\n",
    "    res = convert_to_tokens(indices, tokenizer, extended=extended, extra_values_pos=extra_values_pos)\n",
    "    if with_values:\n",
    "        res = list(zip(res, values.cpu().numpy()))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e03a094-4602-47bf-89ea-0656fb4c1d5d",
   "metadata": {},
   "source": [
    "## Extract Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6e82d2c-92a5-4892-afc3-b40439d6b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"sdadas/polish-gpt2-medium\")\n",
    "tokenizer = my_tokenizer = AutoTokenizer.from_pretrained(\"sdadas/polish-gpt2-medium\")\n",
    "emb = model.get_output_embeddings().weight.data.T.detach()\n",
    "\n",
    "num_layers = model.config.n_layer\n",
    "num_heads = model.config.n_head\n",
    "hidden_dim = model.config.n_embd\n",
    "head_size = hidden_dim // num_heads\n",
    "\n",
    "K = torch.cat([model.get_parameter(f\"transformer.h.{j}.mlp.c_fc.weight\").T\n",
    "                           for j in range(num_layers)]).detach()\n",
    "V = torch.cat([model.get_parameter(f\"transformer.h.{j}.mlp.c_proj.weight\")\n",
    "                           for j in range(num_layers)]).detach()\n",
    "\n",
    "W_Q, W_K, W_V = torch.cat([model.get_parameter(f\"transformer.h.{j}.attn.c_attn.weight\") \n",
    "                           for j in range(num_layers)]).detach().chunk(3, dim=-1)\n",
    "W_O = torch.cat([model.get_parameter(f\"transformer.h.{j}.attn.c_proj.weight\") \n",
    "                           for j in range(num_layers)]).detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "104fd920-f247-4827-be5a-c958dfadbe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_heads = K.reshape(num_layers, -1, hidden_dim)\n",
    "V_heads = V.reshape(num_layers, -1, hidden_dim)\n",
    "d_int = K_heads.shape[1]\n",
    "\n",
    "W_Q_heads = W_Q.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)\n",
    "W_K_heads = W_K.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)\n",
    "W_V_heads = W_V.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)\n",
    "W_O_heads = W_O.reshape(num_layers, num_heads, head_size, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cec5b99d-9e6c-413f-a43d-02fd39485640",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_inv = emb.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58b9e38-8ff4-41f6-8c47-8ae23f4293e6",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b3452-6449-4397-bda7-3209c573c53a",
   "metadata": {},
   "source": [
    "#### Alternative I: No Token List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fbe8ce6-abf5-427b-b374-a43f88b06097",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada3bfc9-3d15-43ae-a21c-da0747b16ba4",
   "metadata": {},
   "source": [
    "#### Alternative II: Can Load Token List from IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bb14a77-5ac2-449b-b771-97be619cb79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fe48f8f-eb62-4bee-a45c-27ff12220825",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = load_dataset(\"clarin-knext/wsd_polish_datasets\", trust_remote_code=True)['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae8eb81a-8525-40ef-8376-a902980c18d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens_num = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea6fd97b-1dbf-4e61-bcdb-f4f2d2c23ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7848/7848 [00:07<00:00, 1059.18it/s]\n"
     ]
    }
   ],
   "source": [
    "if max_tokens_num is None:\n",
    "    tokens_list = set()\n",
    "    for txt in tqdm(imdb):\n",
    "        tokens_list = tokens_list.union(set(tokenizer.tokenize(txt)))\n",
    "else:\n",
    "    tokens_list = Counter()\n",
    "    for txt in tqdm(imdb):\n",
    "        tokens_list.update(set(tokenizer.tokenize(txt)))\n",
    "    tokens_list = map(lambda x: x[0], tokens_list.most_common(max_tokens_num))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2944cdd-13d9-41ea-a2d8-a643ab49b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = set([*map(lambda x: x.strip('Ġ▁').lower(), tokens_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71b325-da96-47b2-99a8-6b909668b642",
   "metadata": {},
   "source": [
    "### FF Keys & Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b891ff3d-18ac-4daf-bb1b-272cc9bb00a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23, 907\n",
      "K          V              -K          -V\n",
      "---------  -------------  ----------  ---------\n",
      "przycu     kody           dotychczas  #bot\n",
      "zalog      #gory          rodzi       #lot\n",
      "wylegi     #ei            do          #dzista\n",
      "#walifik   #zmy           przebie     #remont\n",
      "przesp     Apo            gatunku     #lee\n",
      "pochowany  apokali        przez       #wan\n",
      "#ppe       #128           #ja         #ette\n",
      "wep        ludy           rodzin      #up\n",
      "sfinans    #ords          zrazu       #bul\n",
      "#iss       Cezary         lokalnie    #puszczam\n",
      "#CS        archa          porywa      spu\n",
      "#gny       przy           two         zamyka\n",
      "#zwol      Homo           jeszcze     odstawi\n",
      "#-).       litera         #jaw        #mont\n",
      "lock       #pka           na          #beki\n",
      "skonfisk   Benedykt       wymaga      #laks\n",
      "#post      narodem        ty          tap\n",
      "postoju    polityki       Drze        poby\n",
      "erek       symbolu        pod         posto\n",
      "#ionu      #lachet        Nie         #wana\n",
      "#ppo       publicznego    rzuca       #dzana\n",
      "unierucho  #rzami         ludzkim     #load\n",
      "#ott       anie           uczucia     #owol\n",
      "zbombar    uniwersalny    rze         zatk\n",
      "#FF        artystycznego  tak         opuszcza\n",
      "#prem      ustawowego     da          znajd\n",
      "#klaski    werb           ludzkie     #dzany\n",
      "#gs        wzroku         natury      #elli\n",
      "zatk       #chnienia      #obie       #bek\n",
      "#hony      #osc           pewnego     zrealiz\n"
     ]
    }
   ],
   "source": [
    "i1, i2 = 23, 907\n",
    "# i1, i2 = np.random.randint(num_layers), np.random.randint(d_int)\n",
    "\n",
    "print(f\"{i1}, {i2}\")\n",
    "print(tabulate([*zip(\n",
    "    top_tokens((K_heads[i1, i2]) @ emb, k=30, only_from_list=tokens_list, only_alnum=False),\n",
    "    top_tokens((V_heads[i1, i2]) @ emb, k=30, only_from_list=tokens_list, only_alnum=False),\n",
    "    top_tokens((-K_heads[i1, i2]) @ emb, k=200, only_from_list=tokens_list),\n",
    "    top_tokens((-V_heads[i1, i2]) @ emb, k=200, only_from_list=tokens_list),\n",
    ")], headers=['K', 'V', '-K', '-V']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85067b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91187370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21, 7\n",
      "K            V         -K            -V\n",
      "-----------  --------  ------------  -------------\n",
      "#akija       #czony    #stra         #bing\n",
      "#anel        #bol      #krzy         Broad\n",
      "#aster       #czeni    krzy          #sora\n",
      "#owiak       #jmi      odw           #lki\n",
      "#akuje       #gos      wiesz         #net\n",
      "kulturowego  #fik      panika        Krasi\n",
      "#ader        #sion     szy           turystycznych\n",
      "#akow        #J        #tu           #700\n",
      "#erka        atu       wnie          #lek\n",
      "europy       #rodni    winie         #ing\n",
      "#imes        #wart     #to           #enne\n",
      "#ontent      drabiny   wsu           #zeli\n",
      "#ansen       wiec      #cieka        #1000\n",
      "#ATE         #czona    wyba          #lant\n",
      "#ress        #jskim    si            interna\n",
      "#ords        #andar    rozczarowany  #800\n",
      "Amster       #bra      #kwa          #letki\n",
      "ciagu        magister  mo            #iagno\n",
      "#yne         #jska     #du           #lno\n",
      "#rzak        #roz      zni           Legislacyjne\n",
      "#adem        #bej      tu            zama\n",
      "monitoringu  #Bor      #tum          #202\n",
      "#ations      #jeni     f             #nobi\n",
      "#ated        #borze    ok            #cc\n",
      "#arcia       kad       szyi          #latki\n",
      "#arcie       #cz       znie          #schen\n",
      "generuje     #jski     pow           #owiska\n",
      "#aka         #szlo     #cie          smierci\n",
      "#lacz        rej       oba           net\n",
      "#czuk        #jko      161           #erki\n"
     ]
    }
   ],
   "source": [
    "i1, i2 = 21, 7\n",
    "# i1, i2 = np.random.randint(num_layers), np.random.randint(d_int)\n",
    "\n",
    "print(f\"{i1}, {i2}\")\n",
    "print(tabulate([*zip(\n",
    "    top_tokens((K_heads[i1, i2]) @ emb, k=30, only_from_list=tokens_list, only_alnum=False),\n",
    "    top_tokens((V_heads[i1, i2]) @ emb, k=30, only_from_list=tokens_list, only_alnum=False),\n",
    "    top_tokens((-K_heads[i1, i2]) @ emb, k=200, only_from_list=tokens_list),\n",
    "    top_tokens((-V_heads[i1, i2]) @ emb, k=200, only_from_list=tokens_list),\n",
    ")], headers=['K', 'V', '-K', '-V']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e974a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19, 13\n",
      "K            V           -K         -V\n",
      "-----------  ----------  ---------  ---------------\n",
      "#feld        #yla        wzajemnej  #sekre\n",
      "#ations      #mie        #licz      #arek\n",
      "kanclerza    #Mie        #rzu       #ness\n",
      "#past        #owala      #usa       #bara\n",
      "#finale      #darze      #rozumie   ropy\n",
      "#dzonej      #gil        #Cie       #sat\n",
      "Zachodnim    #torze      #rze       #head\n",
      "#dzone       #jmi        #[         #eco\n",
      "publicznymi  #tul        Szczu      plusy\n",
      "#onny        #widzi      uznaje     dyplomatycznych\n",
      "schodowej    #jami       #twier     Luf\n",
      "technicznym  #Zbigniew   Bi         koncernu\n",
      "kancle       #torom      #roi       #sol\n",
      "#dynki       #zer        #Ka        #ranki\n",
      "#shire       #ciami      Jaku       #arty\n",
      "#stez        Geral       #us        #aryn\n",
      "Gwardii      #wczo       #wu        #feld\n",
      "#dzonymi     #cjonalnie  #try       #ryn\n",
      "#vre         #hama       Mi         belki\n",
      "#dowana      #zowano     uznania    pomara\n",
      "#dzona       #Adam       #MR        bankowego\n",
      "#strzem      #torami     Piet       #ary\n",
      "#manda       #dzieje     Wej        #aster\n",
      "Narodowym    #CZE        Zu         #see\n",
      "czyst        #Gabriel    Mit        #lk\n",
      "#pej         #licz       #Wi        Orlen\n",
      "#technika    #Zale       BI         koncern\n",
      "#meli        #uja        #=         #xt\n",
      "#nickim      #mnie       #Czu       #pek\n",
      "#stad        #bij        #(...)     puszek\n"
     ]
    }
   ],
   "source": [
    "i1, i2 = 19, 13\n",
    "# i1, i2 = np.random.randint(num_layers), np.random.randint(d_int)\n",
    "\n",
    "print(f\"{i1}, {i2}\")\n",
    "print(tabulate([*zip(\n",
    "    top_tokens((K_heads[i1, i2]) @ emb, k=30, only_from_list=tokens_list, only_alnum=False),\n",
    "    top_tokens((V_heads[i1, i2]) @ emb, k=30, only_from_list=tokens_list, only_alnum=False),\n",
    "    top_tokens((-K_heads[i1, i2]) @ emb, k=200, only_from_list=tokens_list),\n",
    "    top_tokens((-V_heads[i1, i2]) @ emb, k=200, only_from_list=tokens_list),\n",
    ")], headers=['K', 'V', '-K', '-V']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "433d1d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20, 9\n",
      "K             V           -K          -V\n",
      "------------  ----------  ----------  ----------\n",
      "#SKY          #pomor      ad          #WG\n",
      "#bioty        #cina       namiest     Eth\n",
      "#ronto        #cin        pogro       #Ger\n",
      "planetoida    #fora       od          #lich\n",
      "#bica         ad          przywo      #Trud\n",
      "#atory        inter       ob          #lowy\n",
      "#alizowane    dziedzi     inter       #lee\n",
      "#Daw          #pina       pona        #RS\n",
      "#KiK          #plika      adiutan     #Is\n",
      "#iT           #kacji      ober        kontynentu\n",
      "#ARS          osobistym   ude         #bytu\n",
      "#Europa       przedmie    pod         #nigdy\n",
      "internetowym  multime     atak        #Stalin\n",
      "internetowy   #gno        zbombar     #niew\n",
      "#alizacji     #rac        zaatak      #lym\n",
      "#TER          #aka        zwo         #Rach\n",
      "#ship         #zdra       nad         #RW\n",
      "#nni          ety         #go         #ARD\n",
      "#elle         #pal        bombar      #lizmie\n",
      "#laby         publicznym  rozstrze    #stoj\n",
      "producentem   #roni       ogro        #bioty\n",
      "#ralni        #roi        po          Pakist\n",
      "#Stali        #pli        pomi        #licki\n",
      "#ott          #tety       dra         #WW\n",
      "#mosfera      odpa        na          #YY\n",
      "#wacji        #piera      zapowiedzi  Dnie\n",
      "#Europej      #cyna       sygna       Barcel\n",
      "#UE           okazjonal   wygna       #Zapad\n",
      "#Oddych       fabry       wie         #stoi\n",
      "#ralizm       #unkt       doktora     PiSu\n"
     ]
    }
   ],
   "source": [
    "i1, i2 = 20, 9\n",
    "\n",
    "print(f\"{i1}, {i2}\")\n",
    "print(tabulate([*zip(\n",
    "    top_tokens((K_heads[i1, i2]) @ emb, k=30, only_from_list=tokens_list, only_alnum=False),\n",
    "    top_tokens((V_heads[i1, i2]) @ emb, k=30, only_from_list=tokens_list, only_alnum=False),\n",
    "    top_tokens((-K_heads[i1, i2]) @ emb, k=200, only_from_list=tokens_list),\n",
    "    top_tokens((-V_heads[i1, i2]) @ emb, k=200, only_from_list=tokens_list),\n",
    ")], headers=['K', 'V', '-K', '-V']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9eae446a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5, 2031\n",
      "K               V          -K           -V\n",
      "--------------  ---------  -----------  -----------\n",
      "Afganistanu     Rzesz      #reb         emerytalne\n",
      "Afga            #parcie    fair         publiczne\n",
      "Jehowy          #tyz       #gle         #szto\n",
      "Wojny           #stie      #cie         ui\n",
      "Obywatelskich   #tz        #bal         #smo\n",
      "zbiorowych      #kop       #patrz       Sub\n",
      "etni            #tkami     #arter       #szard\n",
      "getta           #zdro      #gl          publiczna\n",
      "rdzenia         #lock      moimi        odprawy\n",
      "Kinga           #tto       mym          powsze\n",
      "etnicznych      #TS        #bki         #Ur\n",
      "Afganistanie    #wcze      moim         autorskie\n",
      "#zacje          #ieu       #pi          Anne\n",
      "przegranej      uderzeniu  #de          #mion\n",
      "emerytalnych    rzesz      #lar         integra\n",
      "wojennych       #sci       pas          ur\n",
      "Krajowych       #zej       biurko       publicznych\n",
      "zbiorowe        #zno       #dgo         celne\n",
      "uznanych        hamowania  #dalej       #wymiar\n",
      "mieszkaniowych  #stres     #czu         zawodowo\n",
      "Polsko          #oro       #step        bary\n",
      "nieulecz        Lucas      #skaki       publiczny\n",
      "obywat          szta       #trzym       #owicie\n",
      "podatkowa       #sterze    #dzwo        #Jon\n",
      "granicznych     kalenda    znajdziecie  var\n",
      "wojny           #tek       #rodziej     fel\n",
      "Miriam          #chor      #bek         #rekty\n",
      "afga            diecezji   spostrze     #nom\n",
      "#fikacji        #tora      #ather       Krajowy\n",
      "ubogich         oparciu    #gla         uma\n"
     ]
    }
   ],
   "source": [
    "i1, i2 = 5, 2031\n",
    "\n",
    "print(f\"{i1}, {i2}\")\n",
    "print(tabulate([*zip(\n",
    "    top_tokens((K_heads[i1, i2]) @ emb, k=30, only_from_list=tokens_list, only_alnum=False),\n",
    "    top_tokens((V_heads[i1, i2]) @ emb, k=30, only_from_list=tokens_list, only_alnum=False),\n",
    "    top_tokens((-K_heads[i1, i2]) @ emb, k=200, only_from_list=tokens_list),\n",
    "    top_tokens((-V_heads[i1, i2]) @ emb, k=200, only_from_list=tokens_list),\n",
    ")], headers=['K', 'V', '-K', '-V']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec96387-7cd5-4348-9444-6e1a10987da3",
   "metadata": {},
   "source": [
    "### Attention Weights Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca7323f0-7cf9-4020-bbbd-191de7fe25b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_topk(mat, min_k=500, max_k=250_000, th0=10, max_iters=10, verbose=False):\n",
    "    _get_actual_k = lambda th, th_max: torch.nonzero((mat > th) & (mat < th_max)).shape[0]\n",
    "    th_max = np.inf\n",
    "    left, right = 0, th0 \n",
    "    while True:\n",
    "        actual_k = _get_actual_k(right, th_max)\n",
    "        if verbose:\n",
    "            print(f\"one more iteration. {actual_k}\")\n",
    "        if actual_k <= max_k:\n",
    "            break\n",
    "        left, right = right, right * 2\n",
    "    if min_k <= actual_k <= max_k:\n",
    "        th = right\n",
    "    else:\n",
    "        for _ in range(max_iters):\n",
    "            mid = (left + right) / 2\n",
    "            actual_k = _get_actual_k(mid, th_max)\n",
    "            if verbose:\n",
    "                print(f\"one more iteration. {actual_k}\")\n",
    "            if min_k <= actual_k <= max_k:\n",
    "                break\n",
    "            if actual_k > max_k:\n",
    "                left = mid\n",
    "            else:\n",
    "                right = mid\n",
    "        th = mid\n",
    "    return torch.nonzero((mat > th) & (mat < th_max)).tolist()\n",
    "\n",
    "def get_top_entries(tmp, all_high_pos, only_ascii=False, only_alnum=False, exclude_same=False, exclude_fuzzy=False, tokens_list=None):\n",
    "    remaining_pos = all_high_pos\n",
    "    if only_ascii:\n",
    "        remaining_pos = [*filter(\n",
    "            lambda x: (tokenizer.decode(x[0]).strip('Ġ▁').isascii() and tokenizer.decode(x[1]).strip('Ġ▁').isascii()), \n",
    "            remaining_pos)]\n",
    "    if only_alnum:\n",
    "        remaining_pos = [*filter(\n",
    "            lambda x: (tokenizer.decode(x[0]).strip('Ġ▁ ').isalnum() and tokenizer.decode(x[1]).strip('Ġ▁ ').isalnum()), \n",
    "            remaining_pos)]\n",
    "    if exclude_same:\n",
    "        remaining_pos = [*filter(\n",
    "            lambda x: tokenizer.decode(x[0]).lower().strip() != tokenizer.decode(x[1]).lower().strip(), \n",
    "            remaining_pos)]\n",
    "    if exclude_fuzzy:\n",
    "        remaining_pos = [*filter(\n",
    "            lambda x: not _fuzzy_eq(tokenizer.decode(x[0]).lower().strip(), tokenizer.decode(x[1]).lower().strip()), \n",
    "            remaining_pos)]\n",
    "    if tokens_list:\n",
    "        remaining_pos = [*filter(\n",
    "            lambda x: ((tokenizer.decode(x[0]).strip('Ġ▁').lower().strip() in tokens_list) and \n",
    "                       (tokenizer.decode(x[1]).strip('Ġ▁').lower().strip() in tokens_list)), \n",
    "            remaining_pos)]\n",
    "\n",
    "    pos_val = tmp[[*zip(*remaining_pos)]]\n",
    "    good_cells = [*map(lambda x: (tokenizer.decode(x[0]), tokenizer.decode(x[1])), remaining_pos)]\n",
    "    good_tokens = list(map(lambda x: Counter(x).most_common(), zip(*good_cells)))\n",
    "    remaining_pos_best = np.array(remaining_pos)[torch.argsort(pos_val if reverse_list else -pos_val)[:50]]\n",
    "    good_cells_best = [*map(lambda x: (tokenizer.decode(x[0]), tokenizer.decode(x[1])), remaining_pos_best)]\n",
    "    # good_cells[:100]\n",
    "    # list(zip(good_tokens[0], good_tokens[1]))\n",
    "    return good_cells_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbd4e4a-a69b-4d58-a371-426b9073ff81",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### $W_{VO}$ Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79156807-54af-4fe8-b170-29dc0856d686",
   "metadata": {},
   "source": [
    "Choose **layer** and **head** here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0d00edd-fdca-40d0-807f-e4bf3d7611e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 87\n",
      "one more iteration. 16925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('].', '['),\n",
       " ('],', '['),\n",
       " (' ).', ' ('),\n",
       " (']', '['),\n",
       " ('...),', ' ('),\n",
       " ('przyp', ' ('),\n",
       " (' ),', ' ('),\n",
       " ('-).', ' ('),\n",
       " ('ZOBACZ', ' ['),\n",
       " ('zob', ' ('),\n",
       " ('przyp', ' ['),\n",
       " ('.]', '['),\n",
       " ('%)', ' ('),\n",
       " ('!),', ' ('),\n",
       " ('Ash', ' ('),\n",
       " ('].', ' ['),\n",
       " ('.].', '['),\n",
       " ('],', ' ['),\n",
       " (' rezygnacja', ' ('),\n",
       " ('!).', ' ('),\n",
       " ('prem', ' ('),\n",
       " ('\\x15', ' ('),\n",
       " ('...),', ' (\"'),\n",
       " ('orom', ' ('),\n",
       " ('%),', ' ('),\n",
       " ('ecie', ' ['),\n",
       " ('tul', '['),\n",
       " ('.].', ' ['),\n",
       " ('CZYTAJ', ' ['),\n",
       " ('etu', ' ['),\n",
       " ('-)', ' ('),\n",
       " ('przyp', ' (\"'),\n",
       " ('%).', ' ('),\n",
       " ('?),', ' ('),\n",
       " ('sic', ' ['),\n",
       " ('http', ' ('),\n",
       " (' ]', ' ['),\n",
       " ('Dzwonek', ' ('),\n",
       " (' ).', ' (\"'),\n",
       " ('!)', ' ('),\n",
       " ('demon', ' ('),\n",
       " (' realizacja', ' ('),\n",
       " ('dlaczego', ' ('),\n",
       " ('.]', ' ['),\n",
       " ('Dzwonek', ' (\"'),\n",
       " ('Kodeks', ' ['),\n",
       " ('Bir', ' ['),\n",
       " ('Ash', ' (\"'),\n",
       " ('eu', ' ('),\n",
       " ('przyp', ' (+')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1, i2 = 23, 9\n",
    "i1, i2\n",
    "\n",
    "W_V_tmp, W_O_tmp = W_V_heads[i1, i2, :], W_O_heads[i1, i2]\n",
    "tmp = (emb_inv @ (W_V_tmp @ W_O_tmp) @ emb)\n",
    "\n",
    "all_high_pos = approx_topk(tmp, th0=1, verbose=True) # torch.nonzero((tmp > th) & (tmp < th_max)).tolist()\n",
    "\n",
    "exclude_same = False\n",
    "reverse_list = False\n",
    "only_ascii = True\n",
    "only_alnum = False\n",
    "\n",
    "get_top_entries(tmp, all_high_pos, only_ascii=only_ascii, only_alnum=only_alnum, \n",
    "                exclude_same=exclude_same, tokens_list=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49638ac2-455c-4441-aa67-4fde61c9ea83",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### $W_{QK}$ Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbfc882-3bf7-46e5-8e6a-f6a245ceb7bf",
   "metadata": {},
   "source": [
    "Choose **layer** and **head** here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e74cb0c2-c39f-42ce-a87c-91125207d537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 42\n",
      "one more iteration. 316744\n",
      "one more iteration. 4556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('dnieniem', ' przy'),\n",
       " (' pod', 'Uwiel'),\n",
       " (' w', 'kowicz'),\n",
       " ('aryjnych', ' tylko'),\n",
       " ('aryjnych', ' po'),\n",
       " (' w', 'dagogi'),\n",
       " (' przy', 'owienia'),\n",
       " (' pod', 'nujesz'),\n",
       " (' nad', 'ngu'),\n",
       " (' przy', 'Pry'),\n",
       " (' pod', 'alski'),\n",
       " (' w', 'lujesz'),\n",
       " (' przy', ' budowlane'),\n",
       " ('aryjnych', ' przy'),\n",
       " ('aryjnych', ' nawet'),\n",
       " ('Przytak', ' przy'),\n",
       " (' po', 'bil'),\n",
       " (' od', 'dagogi'),\n",
       " (' po', 'sywnie'),\n",
       " (' do', 'wychw'),\n",
       " ('aryjnych', ' zak'),\n",
       " (' nad', 'obior'),\n",
       " (' pod', 'niuje'),\n",
       " (' pod', 'zc'),\n",
       " (' w', 'towaniem'),\n",
       " (' pod', 'Przytu'),\n",
       " (' przed', 'ingiem'),\n",
       " (' w', 'tujesz'),\n",
       " (' do', 'ariatu'),\n",
       " (' dla', ' pryzmat'),\n",
       " (' pod', ' Wojskowego'),\n",
       " (' po', 'dujesz'),\n",
       " (' z', 'kowicz'),\n",
       " (' na', 'dagogi'),\n",
       " (' w', 'ustu'),\n",
       " (' w', 'lacy'),\n",
       " (' pod', ' podwykona'),\n",
       " (' pod', 'larii'),\n",
       " (' pod', 'tarnego'),\n",
       " (' przy', 'ariacie'),\n",
       " (' na', 'Google'),\n",
       " (' pod', 'Zapra'),\n",
       " (' nad', 'lotu'),\n",
       " (' za', 'nosc'),\n",
       " (' po', 'nujesz'),\n",
       " (' O', 'tyki'),\n",
       " (' po', 'lecie'),\n",
       " (' pod', 'nalnych'),\n",
       " (' pod', ' tery'),\n",
       " (' nad', 'Nad')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1, i2 = 21, 7\n",
    "i1, i2\n",
    "\n",
    "W_Q_tmp, W_K_tmp = W_Q_heads[i1, i2, :], W_K_heads[i1, i2, :]\n",
    "tmp2 = (emb_inv @ (W_Q_tmp @ W_K_tmp.T) @ emb_inv.T)\n",
    "\n",
    "all_high_pos = approx_topk(tmp2, th0=1, verbose=True) # torch.nonzero((tmp2 > th2) & (tmp2 < th_max2)).tolist()\n",
    "\n",
    "exclude_same = False\n",
    "reverse_list = False\n",
    "only_ascii = True\n",
    "only_alnum = True\n",
    "\n",
    "get_top_entries(tmp2, all_high_pos, only_ascii=only_ascii, only_alnum=only_alnum, exclude_same=exclude_same, \n",
    "                tokens_list=tokens_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
