{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, DataCollatorWithPadding, Trainer, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = list(Path(\"./data/ground_truth\").rglob(\"*.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        lines = [json.loads(line) for line in f]\n",
    "    raw_dataset.extend(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_labels(text, labels):\n",
    "    labeled_tokens = []\n",
    "    for label in labels:\n",
    "        if label[2] not in [\"Hate\", \"Neutralny\", \"Mowa nienawiści\"]:\n",
    "            token = text[label[0]: label[1]]\n",
    "            labeled_tokens.append({\"text\": token, \"label\": \"Wzmacnianie\" if label[2] == \"Strenghtening\" else label[2]})\n",
    "    return labeled_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_filtered = []\n",
    "\n",
    "for sample in raw_dataset:\n",
    "    labeled_tokens = get_tokens_labels(sample[\"text\"], sample[\"label\"])\n",
    "    full_dataset_filtered.extend(labeled_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = train_test_split(full_dataset_filtered, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./tokens/train.jsonl\", 'w') as f:\n",
    "    for item in train_ds:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "\n",
    "with open(\"./tokens/test.jsonl\", 'w') as f:\n",
    "    for item in test_ds:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 79 examples [00:00, 79747.30 examples/s]\n",
      "Generating test split: 9 examples [00:00, 10424.95 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 79\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 9\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_files = {\"train\": \"train.jsonl\", \"test\": \"test.jsonl\"}\n",
    "dataset = load_dataset(\"./tokens\", data_files=data_files)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sorted_labels = sorted(set([sample[\"label\"] for sample in train_ds]))\n",
    "label2id = dict(zip(sorted_labels, range(0, len(sorted_labels))))\n",
    "id2label = dict(zip(range(0, len(sorted_labels)), sorted_labels))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-multilingual-cased\",\n",
    "                                                                      num_labels=len(label2id),\n",
    "                                                                      label2id=label2id,\n",
    "                                                                      id2label=id2label)\n",
    "\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 79/79 [00:00<00:00, 16579.94 examples/s]\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 2785.06 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 79\n",
      "}), 'test': Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 9\n",
      "})}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess the dataset by returning tokenized examples.\"\"\"\n",
    "    tokens = tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "    labels = []\n",
    "    for ids, label in zip(tokens[\"input_ids\"], examples[\"label\"]):\n",
    "        labels.append(len(ids)*[label2id[label]])\n",
    "    tokens[\"label\"] = labels\n",
    "    return tokens\n",
    "\n",
    "splits = ['train', 'test']\n",
    "\n",
    "tokenized_ds = {}\n",
    "\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = dataset[split].map(preprocess_function, batched=True)\n",
    "\n",
    "print(tokenized_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): lora.Linear(\n",
      "              (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=768, out_features=64, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=64, out_features=768, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): lora.Linear(\n",
      "              (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=768, out_features=64, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=64, out_features=768, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=64, lora_alpha=1, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "print(peft_model.bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 19/300 [00:00<00:04, 60.21it/s]\n",
      "  7%|▋         | 20/300 [00:00<00:04, 60.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.020595736801624298, 'eval_accuracy': 100.0, 'eval_runtime': 0.0181, 'eval_samples_per_second': 496.061, 'eval_steps_per_second': 165.354, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 40/300 [00:01<00:07, 35.04it/s]\n",
      " 13%|█▎        | 40/300 [00:01<00:07, 35.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.018508905544877052, 'eval_accuracy': 100.0, 'eval_runtime': 0.0181, 'eval_samples_per_second': 497.538, 'eval_steps_per_second': 165.846, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 58/300 [00:01<00:07, 31.59it/s]\n",
      " 20%|██        | 60/300 [00:01<00:07, 31.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.009995183907449245, 'eval_accuracy': 100.0, 'eval_runtime': 0.024, 'eval_samples_per_second': 375.027, 'eval_steps_per_second': 125.009, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 76/300 [00:02<00:07, 30.88it/s]\n",
      " 27%|██▋       | 80/300 [00:02<00:07, 30.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02750464528799057, 'eval_accuracy': 100.0, 'eval_runtime': 0.019, 'eval_samples_per_second': 472.811, 'eval_steps_per_second': 157.604, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 99/300 [00:03<00:05, 34.17it/s]\n",
      " 33%|███▎      | 100/300 [00:03<00:05, 34.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0005763992667198181, 'eval_accuracy': 100.0, 'eval_runtime': 0.0189, 'eval_samples_per_second': 476.343, 'eval_steps_per_second': 158.781, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 117/300 [00:04<00:05, 31.64it/s]\n",
      " 40%|████      | 120/300 [00:04<00:05, 31.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00014649162767454982, 'eval_accuracy': 100.0, 'eval_runtime': 0.0189, 'eval_samples_per_second': 476.451, 'eval_steps_per_second': 158.817, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 136/300 [00:04<00:05, 31.47it/s]\n",
      " 47%|████▋     | 140/300 [00:05<00:05, 31.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0006675662007182837, 'eval_accuracy': 100.0, 'eval_runtime': 0.0175, 'eval_samples_per_second': 514.983, 'eval_steps_per_second': 171.661, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 155/300 [00:05<00:04, 32.31it/s]\n",
      " 53%|█████▎    | 160/300 [00:05<00:04, 32.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5078471733431797e-05, 'eval_accuracy': 100.0, 'eval_runtime': 0.0188, 'eval_samples_per_second': 477.675, 'eval_steps_per_second': 159.225, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 175/300 [00:06<00:03, 32.08it/s]\n",
      " 60%|██████    | 180/300 [00:06<00:03, 32.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2208899534016382e-05, 'eval_accuracy': 100.0, 'eval_runtime': 0.0176, 'eval_samples_per_second': 511.993, 'eval_steps_per_second': 170.664, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 200/300 [00:07<00:02, 37.58it/s]\n",
      " 67%|██████▋   | 200/300 [00:07<00:02, 37.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3511219549400266e-05, 'eval_accuracy': 100.0, 'eval_runtime': 0.0188, 'eval_samples_per_second': 479.965, 'eval_steps_per_second': 159.988, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 219/300 [00:08<00:02, 34.97it/s]\n",
      " 73%|███████▎  | 220/300 [00:08<00:02, 34.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3593637959274929e-05, 'eval_accuracy': 100.0, 'eval_runtime': 0.0177, 'eval_samples_per_second': 507.764, 'eval_steps_per_second': 169.255, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 238/300 [00:08<00:02, 30.71it/s]\n",
      " 80%|████████  | 240/300 [00:08<00:01, 30.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2766617146553472e-05, 'eval_accuracy': 100.0, 'eval_runtime': 0.0182, 'eval_samples_per_second': 493.596, 'eval_steps_per_second': 164.532, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 257/300 [00:09<00:01, 30.80it/s]\n",
      " 87%|████████▋ | 260/300 [00:09<00:01, 30.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2348691598162986e-05, 'eval_accuracy': 100.0, 'eval_runtime': 0.0177, 'eval_samples_per_second': 507.491, 'eval_steps_per_second': 169.164, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 274/300 [00:10<00:01, 25.05it/s]\n",
      " 93%|█████████▎| 280/300 [00:10<00:00, 25.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1786545655922964e-05, 'eval_accuracy': 100.0, 'eval_runtime': 0.0186, 'eval_samples_per_second': 483.357, 'eval_steps_per_second': 161.119, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 295/300 [00:11<00:00, 29.74it/s]\n",
      "100%|██████████| 300/300 [00:12<00:00, 29.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1758585969801061e-05, 'eval_accuracy': 100.0, 'eval_runtime': 0.019, 'eval_samples_per_second': 474.791, 'eval_steps_per_second': 158.264, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:12<00:00, 23.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 12.5196, 'train_samples_per_second': 94.652, 'train_steps_per_second': 23.962, 'train_loss': 0.12991297403971355, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.12991297403971355, metrics={'train_runtime': 12.5196, 'train_samples_per_second': 94.652, 'train_steps_per_second': 23.962, 'total_flos': 8701777841400.0, 'train_loss': 0.12991297403971355, 'epoch': 15.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()*100}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"bert-token-clf\",\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=15,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting to train...\")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
