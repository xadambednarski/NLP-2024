{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukasz/miniconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, GPT2ForTokenClassification, DataCollatorWithPadding, Trainer, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = list(Path(\"./data/ground_truth\").rglob(\"*.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        lines = [json.loads(line) for line in f]\n",
    "    raw_dataset.extend(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_labels(text, labels):\n",
    "    labeled_tokens = []\n",
    "    for label in labels:\n",
    "        if label[2] not in [\"Hate\", \"Neutralny\", \"Mowa nienawiści\"]:\n",
    "            token = text[label[0]: label[1]]\n",
    "            labeled_tokens.append({\"text\": token, \"label\": \"Wzmacnianie\" if label[2] == \"Strenghtening\" else label[2]})\n",
    "    return labeled_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_filtered = []\n",
    "\n",
    "for sample in raw_dataset:\n",
    "    labeled_tokens = get_tokens_labels(sample[\"text\"], sample[\"label\"])\n",
    "    full_dataset_filtered.extend(labeled_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = train_test_split(full_dataset_filtered, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./tokens/train.jsonl\", 'w') as f:\n",
    "    for item in train_ds:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "\n",
    "with open(\"./tokens/test.jsonl\", 'w') as f:\n",
    "    for item in test_ds:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 79 examples [00:00, 49001.78 examples/s]\n",
      "Generating test split: 9 examples [00:00, 8412.91 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 79\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 9\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_files = {\"train\": \"train.jsonl\", \"test\": \"test.jsonl\"}\n",
    "dataset = load_dataset(\"./tokens\", data_files=data_files)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sorted_labels = sorted(set([sample[\"label\"] for sample in train_ds]))\n",
    "label2id = dict(zip(sorted_labels, range(0, len(sorted_labels))))\n",
    "id2label = dict(zip(range(0, len(sorted_labels)), sorted_labels))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2ForTokenClassification.from_pretrained(\"gpt2\",\n",
    "                                                        num_labels=len(label2id),\n",
    "                                                        label2id=label2id,\n",
    "                                                        id2label=id2label)\n",
    "\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Map: 100%|██████████| 79/79 [00:00<00:00, 14883.44 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 2503.40 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 79\n",
      "}), 'test': Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 9\n",
      "})}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess the dataset by returning tokenized examples.\"\"\"\n",
    "    tokens = tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "    labels = []\n",
    "    for ids, label in zip(tokens[\"input_ids\"], examples[\"label\"]):\n",
    "        labels.append(len(ids)*[label2id[label]])\n",
    "    tokens[\"label\"] = labels\n",
    "    return tokens\n",
    "\n",
    "splits = ['train', 'test']\n",
    "\n",
    "tokenized_ds = {}\n",
    "\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = dataset[split].map(preprocess_function, batched=True)\n",
    "\n",
    "print(tokenized_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForTokenClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): lora.Linear(\n",
      "            (base_layer): Conv1D(nf=2304, nx=768)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=64, out_features=2304, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "            (lora_magnitude_vector): ModuleDict()\n",
      "          )\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): ModulesToSaveWrapper(\n",
      "    (original_module): Linear(in_features=768, out_features=3, bias=True)\n",
      "    (modules_to_save): ModuleDict(\n",
      "      (default): Linear(in_features=768, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, r=64, lora_alpha=1, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "print(peft_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:57<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 15/300 [00:00<00:06, 42.33it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "                                                \n",
      "  7%|▋         | 20/300 [00:00<00:06, 42.33it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0018993266858160496, 'eval_accuracy': 100.0, 'eval_runtime': 0.0232, 'eval_samples_per_second': 388.401, 'eval_steps_per_second': 129.467, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 35/300 [00:01<00:07, 33.59it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "                                                \n",
      " 13%|█▎        | 40/300 [00:01<00:07, 33.59it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.000644930056296289, 'eval_accuracy': 100.0, 'eval_runtime': 0.0146, 'eval_samples_per_second': 615.041, 'eval_steps_per_second': 205.014, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 55/300 [00:02<00:09, 26.29it/s]\n",
      "\u001b[A\n",
      "\u001b[A                                    \n",
      "\n",
      "                                                \n",
      " 20%|██        | 60/300 [00:02<00:09, 26.29it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0025543842930346727, 'eval_accuracy': 100.0, 'eval_runtime': 0.0153, 'eval_samples_per_second': 587.328, 'eval_steps_per_second': 195.776, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 75/300 [00:03<00:07, 31.94it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "                                                \n",
      " 27%|██▋       | 80/300 [00:03<00:06, 31.94it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.002782452618703246, 'eval_accuracy': 100.0, 'eval_runtime': 0.0163, 'eval_samples_per_second': 552.553, 'eval_steps_per_second': 184.184, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 95/300 [00:03<00:06, 32.95it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "                                                \n",
      " 33%|███▎      | 100/300 [00:03<00:06, 32.95it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0007988086435943842, 'eval_accuracy': 100.0, 'eval_runtime': 0.0149, 'eval_samples_per_second': 603.555, 'eval_steps_per_second': 201.185, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 115/300 [00:04<00:05, 34.83it/s]\n",
      "\u001b[A\n",
      "\u001b[A                                    \n",
      "\n",
      "                                                 \n",
      " 40%|████      | 120/300 [00:04<00:05, 34.83it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00108518882188946, 'eval_accuracy': 100.0, 'eval_runtime': 0.0171, 'eval_samples_per_second': 526.805, 'eval_steps_per_second': 175.602, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 135/300 [00:05<00:05, 31.54it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "                                                 \n",
      " 47%|████▋     | 140/300 [00:05<00:05, 31.54it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0011629619402810931, 'eval_accuracy': 100.0, 'eval_runtime': 0.0151, 'eval_samples_per_second': 597.498, 'eval_steps_per_second': 199.166, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 155/300 [00:06<00:04, 31.48it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "                                                 \n",
      " 53%|█████▎    | 160/300 [00:06<00:04, 31.48it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0007710996433161199, 'eval_accuracy': 100.0, 'eval_runtime': 0.0159, 'eval_samples_per_second': 567.411, 'eval_steps_per_second': 189.137, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 175/300 [00:06<00:03, 35.25it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "                                                 \n",
      " 60%|██████    | 180/300 [00:06<00:03, 35.25it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00046286990982480347, 'eval_accuracy': 100.0, 'eval_runtime': 0.0154, 'eval_samples_per_second': 583.813, 'eval_steps_per_second': 194.604, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 195/300 [00:07<00:03, 32.73it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "                                                 \n",
      " 67%|██████▋   | 200/300 [00:07<00:03, 32.73it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0003071642422582954, 'eval_accuracy': 100.0, 'eval_runtime': 0.0152, 'eval_samples_per_second': 591.247, 'eval_steps_per_second': 197.082, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 215/300 [00:08<00:02, 34.19it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "                                                 \n",
      " 73%|███████▎  | 220/300 [00:08<00:02, 34.19it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0007199871470220387, 'eval_accuracy': 100.0, 'eval_runtime': 0.0147, 'eval_samples_per_second': 610.258, 'eval_steps_per_second': 203.419, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 235/300 [00:09<00:02, 31.78it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "                                                 \n",
      " 80%|████████  | 240/300 [00:09<00:01, 31.78it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0009816264500841498, 'eval_accuracy': 100.0, 'eval_runtime': 0.016, 'eval_samples_per_second': 561.863, 'eval_steps_per_second': 187.288, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 255/300 [00:09<00:01, 32.50it/s]\n",
      "\u001b[A\n",
      "\u001b[A                                    \n",
      "\n",
      "                                                 \n",
      " 87%|████████▋ | 260/300 [00:09<00:01, 32.50it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0007746624178253114, 'eval_accuracy': 100.0, 'eval_runtime': 0.0152, 'eval_samples_per_second': 591.821, 'eval_steps_per_second': 197.274, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 275/300 [00:10<00:00, 32.80it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "                                                 \n",
      " 93%|█████████▎| 280/300 [00:10<00:00, 32.80it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0006032853270880878, 'eval_accuracy': 100.0, 'eval_runtime': 0.0147, 'eval_samples_per_second': 610.386, 'eval_steps_per_second': 203.462, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 295/300 [00:11<00:00, 31.65it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "                                                 \n",
      "100%|██████████| 300/300 [00:12<00:00, 31.65it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0006287709693424404, 'eval_accuracy': 100.0, 'eval_runtime': 0.0157, 'eval_samples_per_second': 571.725, 'eval_steps_per_second': 190.575, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 300/300 [00:12<00:00, 23.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 12.5855, 'train_samples_per_second': 94.156, 'train_steps_per_second': 23.837, 'train_loss': 0.496390635172526, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.496390635172526, metrics={'train_runtime': 12.5855, 'train_samples_per_second': 94.156, 'train_steps_per_second': 23.837, 'total_flos': 10566444521700.0, 'train_loss': 0.496390635172526, 'epoch': 15.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()*100}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"gpt2-token-clf\",\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=15,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting to train...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
