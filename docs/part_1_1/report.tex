\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[a4paper, margin=1in]{geometry}  
\usepackage{array}  
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage[table]{xcolor}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{float}
\usepackage{breqn}

\title{Anotacja korpusów oraz osadzenia słów i tekstów\\Część I: Procedura anotacji}
\author{Autorzy: Oliwer Krupa, Adam Bednarski, Jan Masłowski, Łukasz Lenkiewicz}
\date{\today}

\begin{document}

\maketitle
\newpage

\renewcommand{\contentsname}{Rozdziały}
\tableofcontents
\newpage

\section{Wybór Korpusu Tekstów}
W celu przeprowadzenia zadania anotacji tekstów wybraliśmy korpus \texttt{poleval2019\_cyberbullying} z HuggingFace Datasets. Korpus ten został opracowany w ramach konkursu PolEval 2019 i zawiera teksty w języku polskim dotyczące problematyki mowy nienawiści i cyberprzemocy. Zbiór został stworzony w celu oceny systemów do detekcji treści o charakterze nienawistnym i przemocowym w internecie. Składa się on z anonimowych postów i komentarzy z polskich mediów społecznościowych, które zostały ręcznie oznaczone pod kątem cyberprzemocy.

Korpus składa się z następujących elementów:
\begin{itemize}
    \item \textbf{Posty i komentarze} - anonimowe wpisy pobrane z różnych platform internetowych.
    \item \textbf{Anotacje} - każdy post został ręcznie zaklasyfikowany jako zawierający lub niezawierający treści związane z mową nienawiści.
\end{itemize}

\subsection{Specyfikacja Zbioru}
Zbiór danych zawiera następujące cechy:
\begin{itemize}
    \item Liczba przykładów: 10,000 wpisów i komentarzy.
    \item Struktura danych: Każdy wpis zawiera pole \texttt{text}, które reprezentuje zawartość tekstową, oraz pole \texttt{label}, które klasyfikuje wpis jako cyberprzemoc (\texttt{1}) lub brak cyberprzemocy (\texttt{0}).
    \item Język: Polski.
\end{itemize}

Więcej informacji na temat zbioru danych znajduje się na stronie projektu: \url{https://huggingface.co/datasets/poleval/poleval2019_cyberbullying}.

\section{Wytyczne i Przeprowadzenie Anotacji Tekstów}

\subsection{Wprowadzenie}
W celu oznaczenia i analizy danych dotyczących mowy nienawiści, zdecydowaliśmy się na wykorzystanie narzędzia \texttt{Docanno}. Docanno umożliwia intuicyjne i efektywne oznaczanie tekstu na różnych poziomach, co pozwala na realizację zadania zarówno na poziomie całych dokumentów, jak i ich fragmentów.

\subsection{Notatka dla Anotatorów}
Aby zapewnić spójność i jednolite podejście podczas procesu anotacji, przygotowaliśmy krótką notatkę zawierającą wytyczne dla anotatorów. Poniżej przedstawiamy instrukcje, które były stosowane przez wszystkie osoby zaangażowane w proces anotacji:

\begin{enumerate}
    \item Oceniamy tweeta w taki sposób, że:
    \begin{itemize}
        \item \texttt{0} - neutralny tweet,
        \item \texttt{1} - mowa nienawiści.
    \end{itemize}
    
    \item Oceniamy frazy tweeta, przypisując im odpowiednie etykiety słowne w zależności od ich wpływu na wydźwięk całego tweeta:
    \begin{itemize}
        \item \texttt{4} - wzmacnianie,
        \item \texttt{5} - odwracanie,
        \item \texttt{6} - osłabianie.
    \end{itemize}
\end{enumerate}

Te wytyczne zapewniają, że anotatorzy zwracają uwagę zarówno na ogólny charakter wpisu, jak i na poszczególne frazy, które mogą mieć wpływ na ton całego tekstu.

\subsection{Proces Anotacji}
W ramach zadania anotacji, każdy anotator został poproszony o oznaczenie 100 wybranych postów, które zostały losowo wybrane z pełnego korpusu danych. Anotacja została przeprowadzona na dwóch poziomach:
\begin{itemize}
    \item \textbf{Anotacja na poziomie całego tekstu} - ocena ogólnego wydźwięku tweeta jako neutralnego lub zawierającego mowę nienawiści.
    \item \textbf{Anotacja na poziomie poszczególnych fragmentów tekstu} - przypisanie odpowiednich etykiet frazom mającym wpływ na wydźwięk tweeta.
\end{itemize}

\subsection{Dobre Praktyki Anotacji}
Podczas procesu anotacji zastosowaliśmy następujące dobre praktyki:
\begin{itemize}
    \item \textbf{Niezależność anotacji} - każdy anotator pracował niezależnie, co zapewnia brak wpływu innych osób na ocenę wpisów.
    \item \textbf{Losowe próbkowanie} - próbka 100 tweetów została losowo wybrana z pełnego zbioru danych, co zwiększa obiektywizm oceny.
    \item \textbf{Klarowne wytyczne} - dzięki jednoznacznym zasadom anotacji, anotatorzy mieli jasność co do sposobu oceny tweetów i ich fragmentów.
\end{itemize}

\subsection{Podsumowanie}
W wyniku procesu anotacji, każdy wpis w próbce został oznaczony zarówno na poziomie całego tekstu, jak i poszczególnych fraz. Wszystkie wyniki anotacji zostały zapisane w plikach JSONL, które zostaną wykorzystane do dalszej analizy. Załączone pliki obejmują oznaczone dane dla każdego anotatora.

\section{Analiza Zgodności Anotatorów}

\subsection{KaPSA Cohena}

W celu oceny zgodności pomiędzy annotatorami w zadaniu klasyfikacji postów obliczono wartość Kappy Cohena. KaPSA Cohena to statystyczna miara zgodności, która uwzględnia nie tylko zgodność rzeczywistą między annotatorami, ale także zgodność przypadkową. Jest to bardziej zaawansowana miara w porównaniu z procentową zgodnością, ponieważ eliminuje wpływ losowości w przypisywaniu kategorii, co czyni ją bardziej miarodajną w analizie wyników.

\paragraph{Macierz konfuzji:}
Na podstawie wyników oznaczania stworzono następującą macierz konfuzji, która pokazuje liczbę przypadków, w których annotatorzy przypisali takie same lub różne etykiety:

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
           & \textbf{Janek: Neutral} & \textbf{Janek: Hate} \\ \hline
\textbf{Adam: Neutral} & 89                      & 4                   \\ \hline
\textbf{Adam: Hate}    & 1                       & 6                   \\ \hline
\end{tabular}
\caption{Macierz konfuzji dla oznaczeń Adama i Janka}
\end{table}

Z tej macierzy wynika, że annotatorzy w 89 przypadkach przypisali kategorię "Neutral", natomiast w 6 przypadkach zgodnie przypisali kategorię "Hate". W 4 przypadkach annotator Adam przypisał "Hate", podczas gdy Janek przypisał "Neutral", natomiast w 1 przypadku było odwrotnie.

\paragraph{Obliczenie Kappy Cohena:}
Funkcja użyta do obliczenia Kappy Cohena bazuje na macierzy konfuzji, która pokazuje zgodności i niezgodności pomiędzy annotatorami. Na podstawie tej macierzy funkcja oblicza, jaka część zgodności wynika z rzeczywistych decyzji annotatorów, a jaka mogła być przypadkowa. Wynik, zwany KaPSA Cohena, przedstawia stopień zgodności po uwzględnieniu przypadkowej zgodności.

KaPSA Cohena w tym przypadku wynosi \( \kaPSA = 0.679 \), co wskazuje na umiarkowaną zgodność pomiędzy annotatorami. Część zgodności można przypisać przypadkowi, ale annotatorzy są w znacznym stopniu zgodni, co czyni wyniki wiarygodnymi, choć nie doskonałymi.

\paragraph{Wyniki:}
Wartości macierzy konfuzji oraz obliczona KaPSA Cohena zostały podsumowane poniżej:

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Wartość}      & \textbf{Opis}                        \\ \hline
\textbf{89}           & Obaj annotatorzy przypisali "Neutral" \\ \hline
\textbf{6}            & Obaj annotatorzy przypisali "Hate"    \\ \hline
\textbf{4}            & Adam przypisał "Hate", Janek "Neutral" \\ \hline
\textbf{1}            & Adam przypisał "Neutral", Janek "Hate" \\ \hline
\textbf{KaPSA Cohena} & 0.679                                 \\ \hline
\end{tabular}
\caption{Wyniki obliczenia Kappy Cohena}
\end{table}

\paragraph{Dyskusja wyników:}
Wartość Kappy Cohena wskazuje na umiarkowaną zgodność annotatorów. Obserwowana zgodność wynosi 95\%, co sugeruje wysoką zgodność pomiędzy annotatorami. Jednak przewidywana zgodność losowa była na poziomie 87\%, co oznacza, że część zgodności mogła wynikać z przypadku. Dlatego wynik \( \kaPSA = 0.679 \) wskazuje, że annotatorzy byli bardziej zgodni, niż wynikałoby to z przypadku, ale ich zgodność nie jest pełna. W związku z tym, aby zwiększyć spójność wyników, wskazane jest przeprowadzenie dalszej analizy oraz ewentualnie kolejnej iteracji anotacji, szczególnie w przypadkach, gdzie niezgodność była wyraźna.



\subsection{Procent Częściowej Zgodności (PSA) dla pierwszej anotacji}

Aby dokładniej ocenić zgodność między anotatorami, obliczono miarę PSA (Positive Specific Agreement). PSA to miara, która uwzględnia stopień częściowej zgodności między dwoma anotatorami podczas etykietowania tekstów lub ich fragmentów. W odróżnieniu od Kappy Cohena, która ocenia ogólną zgodność, PSA skupia się na częściowym pokrywaniu się anotacji, co jest szczególnie użyteczne, gdy mamy do czynienia z wieloma etykietami lub nakładającymi się zakresami.

\subsubsection{PSA dla Całych Zdań}

Na podstawie macierzy konfuzji możemy obliczyć PSA dla anotacji na poziomie zdań:

\begin{itemize}
    \item Adam i Janek zgodzili się w 89 przypadkach co do etykiety „Neutral”.
    \item Zgodzili się w 6 przypadkach co do etykiety „Hate” (mowa nienawiści).
    \item W 4 przypadkach Adam przypisał etykietę „Hate”, podczas gdy Janek ocenił te same zdania jako „Neutral”.
    \item W 1 przypadku Adam przypisał etykietę „Neutral”, a Janek „Hate”.
\end{itemize}

Wzór na obliczenie PSA dla zdań to:

\begin{dmath}
PSA_{\text{zdania}} = \frac{2 \times \text{Zgoda na Neutral i Hate}}{2 \times \text{Zgoda na Neutral i Hate} + \text{Adam: Hate, Janek: Neutral} + \text{Adam: Neutral, Janek: Hate}} \times 100
\end{dmath}

Podstawiając wartości:

\begin{dmath}
PSA_{\text{zdania}} = \frac{2 \times 89}{2 \times 89 + 4 + 1} \times 100 = 97.27\%
\end{dmath}

Wskazuje to na wysoki poziom zgodności między Adamem a Jankiem na poziomie całych zdań.

\subsubsection{PSA dla Frazy}

Następnie przeanalizowano zgodność na poziomie fraz. Po odfiltrowaniu zdań oznaczonych jako „Hate” lub „Neutral”, skupiono się na frazach, takich jak „Wzmacnianie” czy „Osłabianie”, i oceniono, jak często Adam i Janek zgodzili się co do ich etykietowania:

\begin{itemize}
    \item Obaj anotatorzy oznaczyli tę samą frazę: 6 przypadków.
    \item Janek oznaczył frazę, Adam tego nie zrobił: 4 przypadki.
    \item Adam oznaczył frazę, Janek tego nie zrobił: 7 przypadków.
\end{itemize}

Wzór na obliczenie PSA dla fraz to:

\begin{dmath}
PSA_{\text{frazy}} = \frac{2 \times \text{Obaj zgodni}}{2 \times \text{Obaj zgodni} + \text{Janek oznaczył, Adam nie} + \text{Adam oznaczył, Janek nie}} \times 100
\end{dmath}

Podstawiając wartości:

\begin{dmath}
PSA_{\text{frazy}} = \frac{2 \times 6}{2 \times 6 + 4 + 7} \times 100 = 52.17\%
\end{dmath}

Ta niższa wartość odzwierciedla trudności w osiągnięciu pełnej zgodności na bardziej szczegółowym poziomie fraz.

\subsubsection{Średni PSA}

\section{Powtórna Anotacja na Nowej Próbce Danych}

W procesie powtórnej anotacji na nowej próbce danych wprowadzono zaktualizowane wytyczne, mające na celu precyzyjniejszą ocenę treści tweetów oraz ich fraz. Nowa wersja notatki dla anotatorów, oznaczona jako \textbf{Notatka dla Anotatorów 2.0}, zawiera następujące zasady:

\paragraph{1. Ocena ogólna tweetów:}
\begin{itemize}
    \item Tweet jest oceniany na dwóch poziomach:
    \begin{itemize}
        \item \textbf{0} - tweet neutralny, nie zawierający treści związanych z mową nienawiści.
        \item \textbf{1} - tweet zawierający mowę nienawiści.
    \end{itemize}
\end{itemize}

\paragraph{2. Ocena fraz wewnątrz tweetów:}
Po zaklasyfikowaniu tweeta jako zawierającego mowę nienawiści (\textbf{1}), anotator ocenia poszczególne frazy tweeta, biorąc pod uwagę ich wpływ na wydźwięk tweeta:
\begin{itemize}
    \item \textbf{Wzmacnianie (4)} - frazy, które wzmacniają negatywny ton tweeta.
    \item \textbf{Odwracanie (5)} - frazy, które zmieniają kierunek emocjonalny tweeta, łagodząc negatywny ton.
    \item \textbf{Osłabianie (6)} - frazy, które osłabiają negatywny ton tweeta.
\end{itemize}

\paragraph{3. Ograniczenia:}
\begin{itemize}
    \item Wzmacnianie, osłabianie i odwracanie dotyczy tylko tweetów, które zostały zaklasyfikowane jako zawierające mowę nienawiści. W przypadku tweetów neutralnych (0), nie oceniamy wpływu fraz na wydźwięk.
\end{itemize}

Zaktualizowane wytyczne w wersji 2.0 mają na celu bardziej precyzyjną ocenę wpływu poszczególnych fraz w tweetach zawierających mowę nienawiści, co pozwala na głębszą analizę treści i tonu wpisów.

\section{Analiza Zgodności Drugiej Anotacji}

\subsection{KaPSA Cohena}

W celu oceny zgodności pomiędzy annotatorami w drugiej rundzie anotacji obliczono wartość Kappy Cohena. Anotacje zostały przeprowadzone na nowej próbce danych, a wyniki miały na celu sprawdzenie, czy zmiany w wytycznych dla anotatorów wpłynęły na zgodność ich ocen.

\paragraph{Macierz konfuzji:}
Na podstawie wyników anotacji stworzono macierz konfuzji, która przedstawia liczbę przypadków, w których annotatorzy przypisali zgodne lub różne etykiety:

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
           & \textbf{Janek: Neutral} & \textbf{Janek: Hate} \\ \hline
\textbf{Adam: Neutral} & 86                      & 1                   \\ \hline
\textbf{Adam: Hate}    & 3                       & 10                  \\ \hline
\end{tabular}
\caption{Macierz konfuzji dla drugiej anotacji}
\end{table}

\paragraph{Obliczenie Kappy Cohena:}
KaPSA Cohena obliczona na podstawie powyższej macierzy wyniosła \( \kaPSA = 0.811 \). Wynik ten wskazuje na wysoką zgodność między annotatorami, co sugeruje, że zaktualizowane wytyczne były skuteczne w ujednoliceniu ocen.

\paragraph{Wyniki:}
Wynik Kappy Cohena na poziomie 0.811 oznacza znaczną poprawę w porównaniu do pierwszej anotacji. Obserwuje się wysoką zgodność, co może być efektem lepszego zrozumienia i stosowania się do nowych wytycznych przez annotatorów. .

\subsection{Procent Częściowej Zgodności (PSA) dla Drugiej Anotacji}

Dla drugiej rundy anotacji, PSA zostało ponownie obliczone na podstawie danych z plików \texttt{labeled\_sample\_adam\_2.jsonl} oraz \texttt{labeled\_sample\_jan\_2.jsonl}. Podobnie jak w przypadku pierwszej anotacji, obliczono PSA zarówno na poziomie zdań, jak i fraz.

\subsubsection{PSA dla Całych Zdań}

Na podstawie macierzy konfuzji poniżej możemy obliczyć PSA dla zdań:

\[
\text{Macierz konfuzji} =
\begin{bmatrix}
10 & 1 \\
3 & 85 \\
\end{bmatrix}
\]

Macierz ta przedstawia następujące liczby przypisanych etykiet:
\begin{itemize}
    \item 10 przypadków, w których zarówno Adam, jak i Janek przypisali etykietę "Hate".
    \item 85 przypadków, w których obaj anotatorzy przypisali etykietę "Neutral".
    \item 1 przypadek, gdzie Adam przypisał etykietę "Hate", a Janek "Neutral".
    \item 3 przypadki, gdzie Adam przypisał etykietę "Neutral", a Janek "Hate".
\end{itemize}

Wzór na obliczenie PSA dla zdań pozostaje taki sam:

\begin{dmath}
PSA_{\text{zdania}} = \frac{2 \times (10 + 85)}{2 \times (10 + 85) + 1 + 3} \times 100 = 97.27\%
\end{dmath}

Wskazuje to na bardzo wysoki poziom zgodności między anotatorami na poziomie całych zdań, podobnie jak w pierwszej rundzie anotacji.

\subsubsection{PSA dla Frazy}

Po analizie fraz, PSA zostało obliczone na następującej podstawie:
\begin{itemize}
    \item W 8 przypadkach obaj anotatorzy przypisali tę samą etykietę dla fraz.
    \item W 8 przypadkach Janek przypisał etykietę dla frazy, a Adam tego nie zrobił.
    \item W 6 przypadkach Adam przypisał etykietę dla frazy, a Janek tego nie zrobił.
\end{itemize}

Wzór na obliczenie PSA dla fraz to:

\begin{dmath}
PSA_{\text{frazy}} = \frac{2 \times 8}{2 \times 8 + 8 + 6} \times 100 = 53.33\%
\end{dmath}

Wynik PSA dla fraz jest niższy niż w przypadku zdań, co sugeruje większą trudność w osiągnięciu zgodności na bardziej szczegółowym poziomie fraz.

\subsubsection{Średni PSA}

Średnią PSA dla zdań i fraz obliczono w następujący sposób:

\begin{dmath}
PSA_{\text{średnie}} = \frac{PSA_{\text{zdania}} + PSA_{\text{frazy}}}{2} = \frac{97.27 + 53.33}{2} = 75.3\%
\end{dmath}

Średnia zgodność między anotatorami wynosi 75.3\%, co jest wynikiem zbliżonym do pierwszej rundy anotacji, z wyższą zgodnością na poziomie zdań niż fraz.

\section{Wnioski wynikające z przeprowadzonych analiz}

Jednym z głównych problemów, który pojawił się podczas procesu anotacji, była różnica w liczbie zaznaczonych słów przez różnych anotatorów, mimo że ogólny sens anotacji był taki sam. Różnice te wynikały głównie z niejednolitego oznaczania zakresów słów, co utrudniało pełną zgodność.

Analiza wyników wskaźników PSA oraz Kappy Cohena pozwala na wyciągnięcie kilku kluczowych wniosków:

Przeprowadzenie drugiej iteracji anotacji przyniosło wyraźną poprawę zgodności między anotatorami, szczególnie w kontekście oznaczania fraz. Średnia wartość PSA wzrosła z 74,72\% w pierwszej iteracji do 75,3\% w drugiej, co wskazuje na większą spójność w etykietowaniu zarówno całych zdań, jak i ich fragmentów. Zmiany te sugerują, że uaktualnione wytyczne dla anotatorów pomogły lepiej zrozumieć kryteria anotacji, co wpłynęło na poprawę jakości ocen.

Warto podkreślić, że wskaźnik PSA dla anotacji na poziomie zdań pozostał na wysokim poziomie, wynosząc 97,27\% zarówno w pierwszej, jak i drugiej iteracji. Świadczy to o dużej zgodności anotatorów w ocenie całościowego wydźwięku wpisów. Z kolei poprawa PSA dla fraz (z 52,17\% do 53,33\%) wskazuje na pewien postęp w bardziej szczegółowej analizie treści, choć pełna zgodność na poziomie fraz wciąż stanowi wyzwanie.

Podsumowując, wyniki sugerują, że wprowadzenie nowej iteracji anotacji oraz bardziej precyzyjnych instrukcji było krokiem we właściwym kierunku, poprawiającym spójność ocen między anotatorami, szczególnie w analizie bardziej złożonych fragmentów tekstu. Mimo to, dalsze udoskonalenie procesu anotacji może być konieczne, aby zminimalizować rozbieżności w ocenie poszczególnych fragmentów tekstu i osiągnąć jeszcze wyższą zgodność.

\section{Podsumowanie Zbioru Danych za Pomocą Statystyk Opisowych}

W celu lepszego zrozumienia struktury anotacyjnej i charakterystyki tweetów w zbiorze danych, obliczono szereg statystyk opisowych dla każdej grupy anotacji. Statystyki te obejmują liczbę tweetów, średnią i medianę długości wpisów, odchylenie standardowe, a także rozkład wpisów zaklasyfikowanych jako neutralne lub zawierające mowę nienawiści. Analiza tych danych umożliwia ocenę spójności, różnorodności i charakteru próbek danych przypisanych poszczególnym anotatorom.

\subsection{Statystyki dla Anotacji Adama i Jana}

Każdy z anotatorów pracował na niezależnej próbce danych, co oznacza, że ich zestawy tweetów różniły się zarówno pod względem treści, jak i charakterystyki klasyfikacji. W tabelach \ref{tab:adam_stats} i \ref{tab:jan_stats} przedstawiono szczegółowe statystyki dotyczące tweetów oznaczonych przez Adama i Jana, umożliwiające porównanie długości wpisów oraz przydzielonych etykiet.

\begin{table}[h]
    \centering
    \caption{Statystyki dla Anotacji Adama}
    \label{tab:adam_stats}
    \begin{tabular}{|l|l|}
        \hline
        Statystyka & Wartość \\ \hline
        Liczba tweetów & 50 \\ \hline
        Średnia długość (słowa) & 12.39 \\ \hline
        Mediana długości (słowa) & 12.00 \\ \hline
        Odchylenie standardowe & 4.44 \\ \hline
        Najkrótszy wpis (słowa) & 6 \\ \hline
        Najdłuższy wpis (słowa) & 23 \\ \hline
        Liczba tweetów neutralnych & 0 \\ \hline
        Liczba tweetów z mową nienawiści & 0 \\ \hline
        Wpisy z wieloma etykietami & 0 \\ \hline
    \end{tabular}
\end{table}

\begin{table}[h]
    \centering
    \caption{Statystyki dla Anotacji Jana}
    \label{tab:jan_stats}
    \begin{tabular}{|l|l|}
        \hline
        Statystyka & Wartość \\ \hline
        Liczba tweetów & 50 \\ \hline
        Średnia długość (słowa) & 12.44 \\ \hline
        Mediana długości (słowa) & 11.00 \\ \hline
        Odchylenie standardowe & 5.58 \\ \hline
        Najkrótszy tweet (słowa) & 6 \\ \hline
        Najdłuższy tweet (słowa) & 25 \\ \hline
        Liczba tweetów neutralnych & 45 \\ \hline
        Liczba tweetów z mową nienawiści & 5 \\ \hline
        Wpisy z wieloma etykietami & 3 \\ \hline
    \end{tabular}
\end{table}

Zarówno Adam, jak i Jan pracowali na różnych próbkach danych, co skutkuje różnicami w wynikach klasyfikacji. Zbiór anotacji Adama charakteryzuje się brakiem wpisów zaklasyfikowanych jako neutralne lub zawierające mowę nienawiści, co sugeruje, że jego próbka danych mogła składać się z bardziej jednolitych treści. Z kolei Jan, pracując na innej próbie, sklasyfikował 45 tweetów jako neutralne i 5 jako zawierające mowę nienawiści, co wskazuje na większą różnorodność treści w jego zbiorze.

Pomimo różnic w klasyfikacji, długość tweetów w obu próbkach była dość zbliżona, o czym świadczą podobne wartości średniej długości (12.39 słów u Adama i 12.44 słów u Jana). Odchylenie standardowe jest nieco wyższe w przypadku anotacji Jana, co sugeruje większą rozpiętość długości wpisów w jego próbce.

\subsection{Podsumowanie}

Mimo że Adam i Jan pracowali na różnych próbkach danych, można zauważyć kilka wspólnych elementów, takich jak podobna długość tweetów. Główna różnica dotyczy rozkładu etykiet – Adam nie zaklasyfikował żadnych tweetów jako neutralnych lub zawierających mowę nienawiści, natomiast Jan zaklasyfikował znaczną liczbę tweetów jako neutralne i kilka jako zawierające mowę nienawiści.

Wyniki te sugerują, że charakterystyka próbek danych mogła mieć wpływ na decyzje anotatorów. Wprowadzenie lepszych wytycznych w kolejnych iteracjach anotacji mogło pomóc w bardziej spójnej klasyfikacji między anotatorami, jednakże różnice w próbkach danych nadal mogą wpływać na interpretację wyników. W przyszłych analizach warto dążyć do dalszej standaryzacji próbek oraz optymalizacji wytycznych, aby zminimalizować różnice w klasyfikacji wynikające z samego charakteru danych.

\end{document}
